# NetworkÂ ThreatÂ DetectionÂ MLÂ ğŸ›¡ï¸ğŸ“Š

> **Goal:** Train and compare several machineâ€‘learning models that can automatically label each network flow in the **CICIDSâ€‘2017** dataset as **benign** or different types of **attacks**.

![Python](https://img.shields.io/badge/Python-3.9%2B-blue?logo=python)
![License](https://img.shields.io/badge/License-MIT-green)
![Build](https://img.shields.io/badge/Project-Portfolio-success)

---

## âœ¨ Project Highlights
| What | Why it matters |
|------|----------------|
| **Endâ€‘toâ€‘end pipeline** | Goes from raw CSV â†’ cleaned DataFrame â†’ trainedÂ &Â saved models â†’ evaluation metrics. |
| **Model comparison** | Lines upÂ *LogisticÂ Regression* (baseline) against *RandomÂ Forest* and *XGBoost* to show strengths & weaknesses. |
| **Reproducible workflow** | Uses a `requirements.txt`, modular scripts, and joblib checkpoints so anyone can rerun in minutes. |
| **Securityâ€‘domain data** | Works with real IDS traffic from CICIDSâ€‘2017 (or a smaller Kaggleâ€‘hosted CSV) and tackles heavy class imbalance. |

---

## ğŸ—‚ï¸ Repository Layout
~~~text
NetworkThreatDetectionML/
â”œâ”€ data/                  # (ignored) place the CSV here
â”œâ”€ models/                # trained .pkl files (ignored)
â”œâ”€ data_loader.py         # read & preprocess dataset
â”œâ”€ train_models.py        # fit LR, RF, XGB, save to /models
â”œâ”€ evaluate_models.py     # classification report + confusion matrix
â”œâ”€ requirements.txt
â””â”€ README.md
~~~

---

## ğŸ”§ Setup
~~~bash
# 1) Clone the repo
git clone https://github.com/nrileyperez/NetworkThreatDetectionML.git
cd NetworkThreatDetectionML

# 2) Create / activate a virtualâ€‘env (Windows PowerShell example)
python -m venv .venv
.venv\Scripts\activate

# 3) Install dependencies
pip install -r requirements.txt
~~~

---

## ğŸ“¥ Download the dataset
~~~python
import kagglehub, os
os.makedirs("data", exist_ok=True)

# Pull cleaned + preâ€‘processed CSV (~100Â MB)
kagglehub.dataset_download(
    "ericanacletoribeiro/cicids2017-cleaned-and-preprocessed",
    download_path="data",
    unzip=True
)
~~~
*(Or download the ZIP from Kaggle and unzip into `data/` manually.)*

---

## ğŸš€ Usage

### Train all models
~~~bash
python train_models.py
~~~

### Evaluate on the heldâ€‘out test split
~~~bash
python evaluate_models.py
~~~

Example output (abridged):
~~~text
=== random_forest.pkl ===
accuracy  : 0.999
macroÂ F1  : 0.97
           precision recall f1â€‘score support
Attackâ€‘0       0.89   0.77     0.83      384
...
~~~

---

## ğŸ“ˆ Results

| Model | Accuracy | MacroÂ F1 | Notes |
|-------|----------|----------|-------|
| LogisticÂ Regression | 0.89 | 0.34 | Misses rare attacksâ€”high imbalance sensitivity |
| RandomÂ Forest (50Â trees) | **â‰ˆÂ 1.00** | **0.97** | Excellent on all classes; warrants leakage check |
| XGBoost | _(toâ€‘do)_ | _(toâ€‘do)_ | Strong baseline plus feature importances |

---

## ğŸ”® Roadmap / Improvements
- âœ… **Classâ€‘weighting / SMOTE** for the linear model  
- ğŸ”„ Timeâ€‘based or IPâ€‘based train/test split to remove potential leakage  
- ğŸ“Š SHAP explanations & interactive dashboard (Streamlit)  
- â±ï¸ Hyperâ€‘parameter search (`GridSearchCV` / `Optuna`)  
- â˜¸ï¸ Containerised inference endpoint (FastAPI + Docker)

---

## ğŸ¤ Contributing
Issues and PRs are welcome! Feel free to open a discussion if youâ€™d like to experiment with additional algorithms, feature engineering, or deployment tooling.

---

## ğŸ“ License
Distributed under the MIT License. See `LICENSE` for more information.

---

*Built with â¤ï¸ on a RazerÂ BladeÂ StealthÂ 13.*
